{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asia281/rl2023/blob/main/Asia_of_Lab_5_PPO_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://spinningup.openai.com/en/latest/algorithms/ppo.html"
      ],
      "metadata": {
        "id": "PuO3gR5v4eej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L50pF7alrhuM",
        "outputId": "da3b31da-4b2a-412a-98d9-af4443444aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 3.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n",
            "Collecting colabgymrender\n",
            "  Downloading colabgymrender-1.0.9-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.62.3)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.0.9\n",
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.14.3.tar.gz (301 kB)\n",
            "\u001b[K     |████████████████████████████████| 301 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.20.48-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Collecting swagger-spec-validator>=2.7.4\n",
            "  Downloading swagger_spec_validator-2.7.4-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.48\n",
            "  Downloading botocore-1.23.48-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 39.3 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.48->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.10.8)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (3.13)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 50.7 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (4.10.1)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.1.0-py3-none-any.whl (10 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.7.0)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.2)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.19.5)\n",
            "Building wheels for collected packages: neptune-client, future\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.14.3-py2.py3-none-any.whl size=528290 sha256=bc7a9a588fa3aecf749878f75606547a06647e9d8ac3c82579f3e032ce5ed817\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/4f/61/04fa161ac0ed88822161b918b0326c35f487a2692b3016499f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=78314df6dbb93619640883934af13e4b60a97849903581606e7c885b95718a6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built neptune-client future\n",
            "Installing collected packages: arrow, webcolors, urllib3, uri-template, rfc3987, rfc3339-validator, jsonpointer, jmespath, isoduration, fqdn, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.26 PyJWT-2.3.0 arrow-1.2.2 boto3-1.20.48 botocore-1.23.48 bravado-11.0.3 bravado-core-5.17.0 fqdn-1.5.1 future-0.18.2 gitdb-4.0.9 isoduration-20.11.0 jmespath-0.10.0 jsonpointer-2.2 jsonref-0.2 monotonic-1.6 neptune-client-0.14.3 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.5.1 simplejson-3.17.6 smmap-5.0.0 swagger-spec-validator-2.7.4 uri-template-1.1.0 urllib3-1.25.11 webcolors-1.11.1 websocket-client-1.2.3\n",
            "Obtaining spinup_bis from git+https://github.com/awarelab/spinningup_tf2.git#egg=spinup_bis\n",
            "  Cloning https://github.com/awarelab/spinningup_tf2.git to ./src/spinup-bis\n",
            "  Running command git clone -q https://github.com/awarelab/spinningup_tf2.git /content/src/spinup-bis\n",
            "Collecting gym[atari,box2d,classic_control]~=0.15.3\n",
            "  Downloading gym-0.15.7.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from spinup_bis) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from spinup_bis) (3.2.2)\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 42.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spinup_bis) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spinup_bis) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spinup_bis) (1.4.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from spinup_bis) (0.11.2)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.7/dist-packages (from spinup_bis) (2.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup_bis) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup_bis) (1.5.0)\n",
            "Collecting cloudpickle~=1.2.0\n",
            "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: atari_py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup_bis) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup_bis) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup_bis) (4.1.2.30)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[atari,box2d,classic_control]~=0.15.3->spinup_bis) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,box2d,classic_control]~=0.15.3->spinup_bis) (0.18.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (1.13.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (3.10.0.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (0.23.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (1.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (13.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (1.43.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (0.37.1)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->spinup_bis) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0->spinup_bis) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0->spinup_bis) (3.1.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->spinup_bis) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->spinup_bis) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->spinup_bis) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->spinup_bis) (7.6.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->spinup_bis) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->spinup_bis) (5.2.2)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->spinup_bis) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->spinup_bis) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->spinup_bis) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->spinup_bis) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->spinup_bis) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->spinup_bis) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->spinup_bis) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->spinup_bis) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->spinup_bis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->spinup_bis) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->spinup_bis) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->spinup_bis) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->spinup_bis) (1.0.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->spinup_bis) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->spinup_bis) (3.5.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->spinup_bis) (4.9.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->spinup_bis) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->spinup_bis) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->spinup_bis) (5.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->spinup_bis) (21.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->spinup_bis) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->spinup_bis) (0.13.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->spinup_bis) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->spinup_bis) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->spinup_bis) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->spinup_bis) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->spinup_bis) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->spinup_bis) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->spinup_bis) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->spinup_bis) (1.3.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->spinup_bis) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->spinup_bis) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->spinup_bis) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->spinup_bis) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->spinup_bis) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->spinup_bis) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->spinup_bis) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->spinup_bis) (21.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spinup_bis) (2018.9)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->spinup_bis) (2.0.0)\n",
            "Building wheels for collected packages: gym, mpi4py\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-py3-none-any.whl size=1648842 sha256=0625eda126cd6ddcc822eacabc46f861667550403c521bbc0d4e58306a366c67\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/72/05/d3dfcfc2a31bbf886112b6373881bdf2e9e00d2c943f3b4f91\n",
            "  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185313 sha256=3f3e86ab3efce24edcc36789c7c3e54b673dcbe5735b25f7c41e80214abb7b61\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n",
            "Successfully built gym mpi4py\n",
            "Installing collected packages: cloudpickle, gym, mpi4py, spinup-bis\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "  Running setup.py develop for spinup-bis\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.7 mpi4py-3.1.3 spinup-bis-0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle",
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# After the installation is done please restart the colab kernel for it to pick up new packages\n",
        "# No need to rerun this cell after kernel restart\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "!pip install -U colabgymrender\n",
        "!pip install neptune-client \n",
        "!pip install -e git+https://github.com/awarelab/spinningup_tf2.git#egg=spinup_bis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Code of VPG & PPO"
      ],
      "metadata": {
        "id": "wsQZCliDwBct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If this import fails please restart the runtime\n",
        "import spinup_bis"
      ],
      "metadata": {
        "id": "YqbkfV-QvZAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import spinup_bis\n",
        "from spinup_bis.algos.tf2.vpg import core\n",
        "from spinup_bis.utils import logx"
      ],
      "metadata": {
        "id": "6zK-Z_4trih3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer:\n",
        "    \"\"\"A buffer for storing trajectories experienced by a VPG agent.\n",
        "\n",
        "    Uses Generalized Advantage Estimation (GAE-Lambda) for calculating\n",
        "    the advantages of state-action pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
        "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim),\n",
        "                                dtype=np.float32)\n",
        "        self.act_buf = np.zeros(core.combined_shape(size, act_dim),\n",
        "                                dtype=np.float32)\n",
        "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
        "\n",
        "    def store(self, obs, act, rew, val, logp):\n",
        "        \"\"\"Append one timestep of agent-environment interaction to the buffer.\n",
        "        \"\"\"\n",
        "        assert self.ptr < self.max_size  # buffer has to have room\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.act_buf[self.ptr] = act\n",
        "        self.rew_buf[self.ptr] = rew\n",
        "        self.val_buf[self.ptr] = val\n",
        "        self.logp_buf[self.ptr] = logp\n",
        "        self.ptr += 1\n",
        "\n",
        "    def finish_path(self, last_val=0):\n",
        "        \"\"\"Computes reward for unfinished trajectory.\n",
        "\n",
        "        Call this at the end of a trajectory, or when one gets cut off\n",
        "        by an epoch ending. This looks back in the buffer to where the\n",
        "        trajectory started, and uses rewards and value estimates from\n",
        "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
        "        as well as compute the rewards-to-go for each state, to use as\n",
        "        the targets for the value function.\n",
        "        The \"last_val\" argument should be 0 if the trajectory ended\n",
        "        because the agent reached a terminal state (died), and otherwise\n",
        "        should be V(s_T), the value function estimated for the last state.\n",
        "        This allows us to bootstrap the reward-to-go calculation to account\n",
        "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
        "        \"\"\"\n",
        "\n",
        "        path_slice = slice(self.path_start_idx, self.ptr)\n",
        "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "        vals = np.append(self.val_buf[path_slice], last_val)\n",
        "\n",
        "        # the next two lines implement GAE-Lambda advantage calculation\n",
        "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "        self.adv_buf[path_slice] = \\\n",
        "            core.discount_cumsum(deltas, self.gamma * self.lam)\n",
        "\n",
        "        # the next line computes rewards-to-go, targets for the value function\n",
        "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
        "\n",
        "        self.path_start_idx = self.ptr\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"Returns data stored in buffer.\n",
        "\n",
        "        Call this at the end of an epoch to get all of the data from\n",
        "        the buffer, with advantages appropriately normalized (shifted to have\n",
        "        mean zero and std one). Also, resets some pointers in the buffer.\n",
        "        \"\"\"\n",
        "        assert self.ptr == self.max_size  # buffer has to be full\n",
        "        self.ptr, self.path_start_idx = 0, 0\n",
        "        # the next two lines implement the advantage normalization trick\n",
        "        adv_mean, adv_std = self.adv_buf.mean(), self.adv_buf.std()\n",
        "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "        return [self.obs_buf, self.act_buf, self.adv_buf,\n",
        "                self.ret_buf, self.logp_buf]"
      ],
      "metadata": {
        "id": "NK7eU70Wr7mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyGradientAlgorithm(abc.ABC):\n",
        "    def __init__(self, env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=None, seed=0,\n",
        "            total_steps=int(1e6), train_every=4000, log_every=4000, gamma=0.99,\n",
        "            pi_lr=3e-4, v_lr=1e-3, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
        "            logger_kwargs=None):\n",
        "        \"\"\"Vanilla Policy Gradient with GAE-Lambda for advantage estimation.\n",
        "\n",
        "        Args:\n",
        "            env_fn : A function which creates a copy of the environment.\n",
        "                The environment must satisfy the OpenAI Gym API.\n",
        "\n",
        "            actor_critic: A function which takes in `action_space` and\n",
        "                `observation_space` kwargs, and returns actor and critic\n",
        "                tf.keras.Model-s.\n",
        "\n",
        "            ac_kwargs (dict): Any kwargs appropriate for the actor_critic\n",
        "                function you provided to VPG.\n",
        "\n",
        "            seed (int): Seed for random number generators.\n",
        "\n",
        "            total_steps (int): Number of environment interactions to run and train\n",
        "                the agent.\n",
        "\n",
        "            train_every (int): Number of environment interactions that should elapse\n",
        "                between training epochs.\n",
        "\n",
        "            log_every (int): Number of environment interactions that should elapse\n",
        "                between dumping logs.\n",
        "\n",
        "            gamma (float): Discount factor. (Always between 0 and 1.)\n",
        "\n",
        "            pi_lr (float): Learning rate for policy optimizer.\n",
        "\n",
        "            v_lr (float): Learning rate for value function optimizer.\n",
        "\n",
        "            train_v_iters (int): Number of gradient descent steps to take on\n",
        "                value function per epoch.\n",
        "\n",
        "            lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,\n",
        "                close to 1.)\n",
        "\n",
        "            max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
        "\n",
        "            logger_kwargs (dict): Keyword args for EpochLogger.\n",
        "        \"\"\"\n",
        "        params_to_save = locals()\n",
        "        self.logger = logx.EpochLogger(**(logger_kwargs or {}))\n",
        "        self.logger.save_config(params_to_save)\n",
        "\n",
        "        random.seed(seed)\n",
        "        tf.random.set_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # In case of distributed computations these values have to be updated\n",
        "        self.total_steps = total_steps\n",
        "        self.train_every = train_every\n",
        "        self.log_every = log_every\n",
        "        self.max_ep_len = max_ep_len\n",
        "        self.train_v_iters = train_v_iters\n",
        "\n",
        "        self.env = env_fn()\n",
        "        obs_dim = np.prod(self.env.observation_space.shape)\n",
        "        act_dim = self.env.action_space.shape\n",
        "        self.env.seed(seed)\n",
        "\n",
        "        self.replay_buffer = Buffer(obs_dim=obs_dim, act_dim=act_dim,\n",
        "                                size=train_every, gamma=gamma, lam=lam)\n",
        "\n",
        "        ac_kwargs = ac_kwargs or {}\n",
        "        ac_kwargs['observation_space'] = self.env.observation_space\n",
        "        ac_kwargs['action_space'] = self.env.action_space\n",
        "\n",
        "        self.actor, self.critic = actor_critic(**ac_kwargs)\n",
        "\n",
        "        self.actor.build(input_shape=(None, obs_dim))\n",
        "        self.critic.build(input_shape=(None, obs_dim))\n",
        "\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=pi_lr)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=v_lr)\n",
        "\n",
        "    @tf.function\n",
        "    def value(self, observations):\n",
        "        return self.critic(observations)\n",
        "\n",
        "    def get_value(self, observation):\n",
        "        return self.value(np.array([observation])).numpy()[0]\n",
        "\n",
        "    @tf.function\n",
        "    def value_loss(self, observations, rtg):\n",
        "        return tf.reduce_mean((self.critic(observations) - rtg) ** 2)\n",
        "\n",
        "    @tf.function\n",
        "    def value_train_step(self, observations, rtg):\n",
        "        def loss():\n",
        "            return self.value_loss(observations, rtg)\n",
        "\n",
        "        self.critic_optimizer.minimize(loss, self.critic.trainable_variables)\n",
        "\n",
        "        return loss()\n",
        "\n",
        "    def get_action(self, observation):\n",
        "        return self.actor.action(np.array([observation])).numpy()[0]\n",
        "\n",
        "    @tf.function\n",
        "    def pi_train_step(self, observations, actions, advantages, logp_old):\n",
        "        def loss():\n",
        "            logp = self.actor.action_logprob(observations, actions)\n",
        "            return self.pi_loss(logp, logp_old, advantages)\n",
        "\n",
        "        self.actor_optimizer.minimize(loss, self.actor.trainable_variables)\n",
        "\n",
        "        # For logging purposes\n",
        "        logp = self.actor.action_logprob(observations, actions)\n",
        "        loss_new = self.pi_loss(logp, logp_old, advantages)\n",
        "\n",
        "        return loss_new, tf.reduce_mean(logp_old - logp), tf.reduce_mean(-logp)\n",
        "\n",
        "    def run(self):\n",
        "        start_time = time.time()\n",
        "        obs, ep_ret, ep_len = self.env.reset(), 0, 0\n",
        "\n",
        "        # Main loop: collect experience in env and update/log each epoch\n",
        "        for t in range(self.total_steps):\n",
        "            action = self.get_action(obs)\n",
        "            v_t = self.get_value(obs)\n",
        "            logp = self.actor.action_logprob(np.array([obs]),\n",
        "                                        np.array([action])).numpy()[0]\n",
        "\n",
        "            # Step the env\n",
        "            new_obs, rew, done, _ = self.env.step(action)\n",
        "            ep_ret += rew\n",
        "            ep_len += 1\n",
        "\n",
        "            # Ignore the \"done\" signal if it comes from hitting the time\n",
        "            # horizon (that is, when it's an artificial terminal signal\n",
        "            # that isn't based on the agent's state)\n",
        "            done = False if ep_len == self.max_ep_len else done\n",
        "\n",
        "            # Store experience to replay buffer\n",
        "            self.replay_buffer.store(obs, action, rew, v_t, logp)\n",
        "            self.logger.store(VVals=v_t)\n",
        "\n",
        "            # Super critical, easy to overlook step: make sure to update\n",
        "            # most recent observation!\n",
        "            obs = new_obs\n",
        "\n",
        "            # End of trajectory handling\n",
        "            if done or (ep_len == self.max_ep_len):\n",
        "                self.logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
        "\n",
        "            if done or (ep_len == self.max_ep_len) or (t + 1) % self.train_every == 0:\n",
        "                obs, ep_ret, ep_len = self.env.reset(), 0, 0\n",
        "\n",
        "                # if trajectory didn't reach terminal state, bootstrap value target\n",
        "                last_val = 0 if done else self.get_value(obs)\n",
        "                self.replay_buffer.finish_path(last_val)\n",
        "\n",
        "            # Update handling\n",
        "            if (t + 1) % self.train_every == 0:\n",
        "                self.do_update()\n",
        "\n",
        "            # End of epoch wrap-up\n",
        "            if ((t + 1) % self.log_every == 0) or (t + 1 == self.total_steps):\n",
        "                # Log info about epoch\n",
        "                self.logger.log_tabular('EpRet', with_min_and_max=True)\n",
        "                self.logger.log_tabular('EpLen', average_only=True)\n",
        "                self.logger.log_tabular('VVals', with_min_and_max=True)\n",
        "                self.logger.log_tabular('LossV', average_only=True)\n",
        "                self.logger.log_tabular('LossPi', average_only=True)\n",
        "                self.logger.log_tabular('Entropy', average_only=True)\n",
        "                self.logger.log_tabular('KL', average_only=True)\n",
        "                self.logger.log_tabular('TotalEnvInteracts', (t + 1))\n",
        "                self.logger.log_tabular('Time', time.time() - start_time)\n",
        "                self.logger.dump_tabular()\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def do_update(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def pi_loss(self, logp, advantages):\n",
        "        pass"
      ],
      "metadata": {
        "id": "oA0bl67HwFlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "Please fill in missing pieces of code marked:\n",
        "```\n",
        "###### TODO ######\n",
        "<Descritpion>\n",
        "#-----------------\n",
        "\n",
        "###### END  ######\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "W0vR7OhAxUbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VPG(PolicyGradientAlgorithm):\n",
        "    def do_update(self):\n",
        "        [batch_obs, batch_act, batch_adv, batch_rtg,\n",
        "        batch_logp] = self.replay_buffer.get()\n",
        "\n",
        "        loss, kl, entropy = self.pi_train_step(batch_obs, batch_act,\n",
        "                                        batch_adv, batch_logp)\n",
        "        self.logger.store(LossPi=loss.numpy(), KL=kl.numpy(),\n",
        "                    Entropy=entropy.numpy())\n",
        "\n",
        "        for _ in range(self.train_v_iters):\n",
        "            loss = self.value_train_step(batch_obs, batch_rtg)\n",
        "            self.logger.store(LossV=loss)\n",
        "\n",
        "    @tf.function\n",
        "    def pi_loss(self, logp, _logp_old, advantages):\n",
        "        ###### TODO ######\n",
        "        # Implement pi loss calcualation \n",
        "        #-----------------\n",
        "        loss = \n",
        "        ###### END  ######\n",
        "        return loss\n",
        "\n",
        "\n",
        "class PPO(PolicyGradientAlgorithm):\n",
        "    def __init__(self, clip_ratio=0.2, train_pi_iters=80, target_kl=0.01, **kwargs):\n",
        "        super(PPO, self).__init__(**kwargs)\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.train_pi_iters = train_pi_iters\n",
        "        self.target_kl = target_kl\n",
        "\n",
        "    def do_update(self):\n",
        "        [batch_obs, batch_act, batch_adv, batch_rtg,\n",
        "            batch_logp] = self.replay_buffer.get()\n",
        "\n",
        "        for i in range(self.train_pi_iters):\n",
        "            loss, kl, entropy = self.pi_train_step(batch_obs, batch_act,\n",
        "                                                batch_adv, batch_logp)\n",
        "            self.logger.store(LossPi=loss.numpy(), KL=kl.numpy(),\n",
        "                            Entropy=entropy.numpy())\n",
        "            \n",
        "            ###### TODO ######\n",
        "            # Implement early stopping of pi training based on kl value\n",
        "            # Log step at which you break the loop using self.logger.log method\n",
        "            #-----------------\n",
        "            if kl ...\n",
        "            ###### END  ######\n",
        "\n",
        "        for _ in range(self.train_v_iters):\n",
        "            loss = self.value_train_step(batch_obs, batch_rtg)\n",
        "            self.logger.store(LossV=loss)\n",
        "\n",
        "    @tf.function\n",
        "    def pi_loss(self, logp, logp_old, advantages):\n",
        "        ###### TODO ######\n",
        "        # Implement pi loss calcualation\n",
        "        #-----------------\n",
        "        \n",
        "        loss = \n",
        "        ###### END  ######\n",
        "        return loss "
      ],
      "metadata": {
        "id": "tRqIRqpewJeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets test the solution - train the agents"
      ],
      "metadata": {
        "id": "yXwLV3wZwQ3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First sanity check on CartPole\n",
        "If your implementation cannot reach the goals specified below you might have some bug in your code."
      ],
      "metadata": {
        "id": "MmOszbEms94Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "from spinup_bis.algos.tf2.vpg.core import mlp_actor_critic\n",
        "from spinup_bis import ppo_tf2 as PPO_org\n",
        "\n",
        "def env_fn():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    return env"
      ],
      "metadata": {
        "id": "yoh4ditwwSfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VPG should reach performance over 100 with this setting \n",
        "vpg = VPG(env_fn=env_fn, actor_critic=mlp_actor_critic, total_steps=int(5e+4), v_lr=0.001, pi_lr=0.0003, train_every=500, max_ep_len=500, logger_kwargs={\"output_dir\": \"./logs/vpg\"})\n",
        "vpg.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8ksSmNhtNgm",
        "outputId": "9a031b73-c845-4ab1-a9e6-d553d8fc7e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Log dir ./logs/vpg already exists! Storing info there anyway.\n",
            "\u001b[32;1mLogging data to ./logs/vpg/progress.txt\u001b[0m\n",
            "\u001b[36;1mSaving config:\n",
            "\u001b[0m\n",
            "{\n",
            "    \"ac_kwargs\":\tnull,\n",
            "    \"actor_critic\":\t\"mlp_actor_critic\",\n",
            "    \"env_fn\":\t\"env_fn\",\n",
            "    \"gamma\":\t0.99,\n",
            "    \"lam\":\t0.97,\n",
            "    \"log_every\":\t4000,\n",
            "    \"logger_kwargs\":\t{\n",
            "        \"output_dir\":\t\"./logs/vpg\"\n",
            "    },\n",
            "    \"max_ep_len\":\t500,\n",
            "    \"pi_lr\":\t0.0003,\n",
            "    \"seed\":\t0,\n",
            "    \"self\":\t{\n",
            "        \"<__main__.VPG object at 0x7f8a80912a10>\":\t{\n",
            "            \"logger\":\t{\n",
            "                \"<spinup_bis.utils.logx.EpochLogger object at 0x7f8a80912c50>\":\t{\n",
            "                    \"epoch_dict\":\t{},\n",
            "                    \"exp_name\":\tnull,\n",
            "                    \"first_row\":\ttrue,\n",
            "                    \"log_current_row\":\t{},\n",
            "                    \"log_headers\":\t[],\n",
            "                    \"neptune_run\":\tnull,\n",
            "                    \"output_dir\":\t\"./logs/vpg\",\n",
            "                    \"output_file\":\t{\n",
            "                        \"<_io.TextIOWrapper name='./logs/vpg/progress.txt' mode='w' encoding='UTF-8'>\":\t{\n",
            "                            \"mode\":\t\"w\"\n",
            "                        }\n",
            "                    }\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"total_steps\":\t50000,\n",
            "    \"train_every\":\t500,\n",
            "    \"train_v_iters\":\t80,\n",
            "    \"v_lr\":\t0.001\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "|      AverageEpRet |            25.9 |\n",
            "|          StdEpRet |            15.5 |\n",
            "|          MaxEpRet |              98 |\n",
            "|          MinEpRet |               8 |\n",
            "|             EpLen |            25.9 |\n",
            "|      AverageVVals |            11.4 |\n",
            "|          StdVVals |            6.11 |\n",
            "|          MaxVVals |            18.7 |\n",
            "|          MinVVals |           -11.7 |\n",
            "|             LossV |             157 |\n",
            "|            LossPi |         -0.0137 |\n",
            "|           Entropy |           0.688 |\n",
            "|                KL |        6.51e-05 |\n",
            "| TotalEnvInteracts |           4e+03 |\n",
            "|              Time |            16.4 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|      AverageEpRet |            31.5 |\n",
            "|          StdEpRet |            19.9 |\n",
            "|          MaxEpRet |             108 |\n",
            "|          MinEpRet |               8 |\n",
            "|             EpLen |            31.5 |\n",
            "|      AverageVVals |            18.8 |\n",
            "|          StdVVals |            5.67 |\n",
            "|          MaxVVals |            24.6 |\n",
            "|          MinVVals |           -16.5 |\n",
            "|             LossV |             165 |\n",
            "|            LossPi |         -0.0239 |\n",
            "|           Entropy |           0.676 |\n",
            "|                KL |        2.32e-05 |\n",
            "| TotalEnvInteracts |           8e+03 |\n",
            "|              Time |            31.7 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|      AverageEpRet |            36.8 |\n",
            "|          StdEpRet |            20.1 |\n",
            "|          MaxEpRet |             120 |\n",
            "|          MinEpRet |              11 |\n",
            "|             EpLen |            36.8 |\n",
            "|      AverageVVals |            19.9 |\n",
            "|          StdVVals |            8.28 |\n",
            "|          MaxVVals |            30.9 |\n",
            "|          MinVVals |          -0.117 |\n",
            "|             LossV |             123 |\n",
            "|            LossPi |         -0.0354 |\n",
            "|           Entropy |           0.661 |\n",
            "|                KL |       -0.000139 |\n",
            "| TotalEnvInteracts |         1.2e+04 |\n",
            "|              Time |              47 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|      AverageEpRet |              42 |\n",
            "|          StdEpRet |            20.9 |\n",
            "|          MaxEpRet |              99 |\n",
            "|          MinEpRet |              10 |\n",
            "|             EpLen |              42 |\n",
            "|      AverageVVals |            22.7 |\n",
            "|          StdVVals |            9.14 |\n",
            "|          MaxVVals |            36.3 |\n",
            "|          MinVVals |            1.45 |\n",
            "|             LossV |             122 |\n",
            "|            LossPi |         -0.0399 |\n",
            "|           Entropy |           0.651 |\n",
            "|                KL |       -8.88e-05 |\n",
            "| TotalEnvInteracts |         1.6e+04 |\n",
            "|              Time |            62.1 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|      AverageEpRet |            45.2 |\n",
            "|          StdEpRet |            22.8 |\n",
            "|          MaxEpRet |             154 |\n",
            "|          MinEpRet |              12 |\n",
            "|             EpLen |            45.2 |\n",
            "|      AverageVVals |            24.3 |\n",
            "|          StdVVals |            11.4 |\n",
            "|          MaxVVals |            41.1 |\n",
            "|          MinVVals |            1.16 |\n",
            "|             LossV |             124 |\n",
            "|            LossPi |         -0.0487 |\n",
            "|           Entropy |           0.648 |\n",
            "|                KL |        0.000157 |\n",
            "| TotalEnvInteracts |           2e+04 |\n",
            "|              Time |            77.3 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|      AverageEpRet |            51.2 |\n",
            "|          StdEpRet |            24.7 |\n",
            "|          MaxEpRet |             121 |\n",
            "|          MinEpRet |              15 |\n",
            "|             EpLen |            51.2 |\n",
            "|      AverageVVals |            25.9 |\n",
            "|          StdVVals |            11.6 |\n",
            "|          MaxVVals |            44.8 |\n",
            "|          MinVVals |           -6.83 |\n",
            "|             LossV |             160 |\n",
            "|            LossPi |         -0.0385 |\n",
            "|           Entropy |            0.64 |\n",
            "|                KL |        0.000135 |\n",
            "| TotalEnvInteracts |         2.4e+04 |\n",
            "|              Time |            92.5 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|      AverageEpRet |            60.3 |\n",
            "|          StdEpRet |              35 |\n",
            "|          MaxEpRet |             156 |\n",
            "|          MinEpRet |              14 |\n",
            "|             EpLen |            60.3 |\n",
            "|      AverageVVals |            30.4 |\n",
            "|          StdVVals |            13.2 |\n",
            "|          MaxVVals |            49.8 |\n",
            "|          MinVVals |           -3.05 |\n",
            "|             LossV |             202 |\n",
            "|            LossPi |         -0.0375 |\n",
            "|           Entropy |           0.632 |\n",
            "|                KL |        6.86e-05 |\n",
            "| TotalEnvInteracts |         2.8e+04 |\n",
            "|              Time |             108 |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "|      AverageEpRet |            75.3 |\n",
            "|          StdEpRet |            40.8 |\n",
            "|          MaxEpRet |             168 |\n",
            "|          MinEpRet |              18 |\n",
            "|             EpLen |            75.3 |\n",
            "|      AverageVVals |              31 |\n",
            "|          StdVVals |              15 |\n",
            "|          MaxVVals |            57.6 |\n",
            "|          MinVVals |           -13.2 |\n",
            "|             LossV |             257 |\n",
            "|            LossPi |         -0.0386 |\n",
            "|           Entropy |           0.623 |\n",
            "|                KL |       -6.72e-05 |\n",
            "| TotalEnvInteracts |         3.2e+04 |\n",
            "|              Time |             123 |\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO should reach performance around 500 with this setting\n",
        "ppo = PPO(env_fn=env_fn, actor_critic=mlp_actor_critic, total_steps=int(5e+4), v_lr=0.001, pi_lr=0.0003, train_every=500, max_ep_len=500, logger_kwargs={\"output_dir\": \"./logs/ppo\"})\n",
        "ppo.run()"
      ],
      "metadata": {
        "id": "4_Q5fIphtOne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now lets learn more difficult task"
      ],
      "metadata": {
        "id": "N6Wb92FqtO9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def env_fn():\n",
        "    env = gym.make(\"BipedalWalker-v3\")\n",
        "    return env"
      ],
      "metadata": {
        "id": "0zitcMyPtD1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vpg = VPG(env_fn=env_fn, actor_critic=mlp_actor_critic, total_steps=int(5e+5), v_lr=0.001, pi_lr=0.0003, train_every=1000, logger_kwargs={\"output_dir\": \"./logs/vpg\"})\n",
        "vpg.run()"
      ],
      "metadata": {
        "id": "83yIe19Ki1r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo = PPO(env_fn=env_fn, actor_critic=mlp_actor_critic,  total_steps=int(5e+5), v_lr=0.001, pi_lr=0.0003, train_every=1000, logger_kwargs={\"output_dir\": \"./logs/ppo\"})\n",
        "ppo.run()"
      ],
      "metadata": {
        "id": "IvCa19ctizPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize learned policy"
      ],
      "metadata": {
        "id": "xJri7TqGwiYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "nEnc2Zj4wkqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(actor):\n",
        "    NUM_TEST_RUNS = 10\n",
        "    env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "    performance = []\n",
        "    for _ in range(NUM_TEST_RUNS):\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0\n",
        "        while not done:\n",
        "            action = actor(tf.expand_dims(obs, axis=0))[0]\n",
        "            obs, reward, done, info = env.step(action.numpy()[0])\n",
        "            ep_reward += reward\n",
        "        performance.append(ep_reward)\n",
        "\n",
        "    env = gym.make(\"BipedalWalker-v3\")\n",
        "    env = Recorder(env, \"./videos\")\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        action = actor(tf.expand_dims(obs, axis=0))[0]\n",
        "        obs, reward, done, info = env.step(action.numpy()[0])\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.release()\n",
        "    env.play()\n",
        "\n",
        "    return np.asarray(performance)"
      ],
      "metadata": {
        "id": "_RlWB-AGz0tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vpg_performance = test_agent(vpg.actor)"
      ],
      "metadata": {
        "id": "C23QICB9k1sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_performance = test_agent(ppo.actor)"
      ],
      "metadata": {
        "id": "Rj91VwGXlLHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate performance"
      ],
      "metadata": {
        "id": "wHg_lCdql16K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vpg_data = pd.read_csv(\"./logs/vpg/progress.txt\", delimiter=\"\\t\")\n",
        "ppo_data = pd.read_csv(\"./logs/ppo/progress.txt\", delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "pvXle7pERF7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(vpg_data[\"AverageEpRet\"], label=\"VPG\")\n",
        "plt.plot(ppo_data[\"AverageEpRet\"], label=\"PPO\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m9ry3AGSROBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar([\"vpg\", \"ppo\"], [vpg_performance.mean(), ppo_performance.mean()], yerr=[vpg_performance.std(), ppo_performance.std()])"
      ],
      "metadata": {
        "id": "eC0OEa1JRdDf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}