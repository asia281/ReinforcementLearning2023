{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asia281/rl2023/blob/main/Asia_of_Lab_09_Imitation_Learning_(with_gaps).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ztK8xPJk8U"
      },
      "source": [
        "<center><img src='https://i.postimg.cc/TPR1n1rp/AI-Tech-PL-RGB.png' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://i.postimg.cc/Gpq2KRQz/logotypy-aitech.jpg'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\" \n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3LyMs4XezwL"
      },
      "source": [
        "# Lab 03: Imitation Learning\n",
        "\n",
        "In this lab, we look into the problem of learning from expert demonstrations.\n",
        "\n",
        "- Find a policy $\\pi(a | s)$ that best imitates the expert policy $\\pi^*(a | s)$ in the given environment.\n",
        "- It's worth noting, that we don't need access to the environment rewards.\n",
        "\n",
        "Major Imitation Learning techniques are:\n",
        "\n",
        "1. Behavioural Cloning,\n",
        "1. Imitation Learning via Interactive Demonstrator e.g. SMILe (Ross and Bagnell, 2010) or DAgger (Ross et al., 2011),\n",
        "1. Inverse Reinforcement Learning -- out of scope of this lab.\n",
        "\n",
        "We will solve the Ant problem, shown below, examining the first two approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vmZNWiS_jA9-"
      },
      "outputs": [],
      "source": [
        "#@title Mount your Google Drive\n",
        "\n",
        "#@markdown Your work will be stored in a folder called `rl_lab_2022` by default.\n",
        "\n",
        "#@markdown Run each section with Shift+Enter\n",
        "\n",
        "#@markdown Double-click on section headers to show code.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "LAB_PATH = '/content/gdrive/MyDrive/rl_lab_2022/imitation_learning'\n",
        "if not os.path.exists(LAB_PATH):\n",
        "  %mkdir -p $LAB_PATH\n",
        "\n",
        "MJC_PATH = '{}/mujoco'.format(LAB_PATH)\n",
        "if not os.path.exists(MJC_PATH):\n",
        "    %mkdir $MJC_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ACWD97lNxPU"
      },
      "outputs": [],
      "source": [
        "#@title Install requirements\n",
        "\n",
        "!apt -q update \n",
        "!apt install -q -y --no-install-recommends \\\n",
        "        build-essential \\\n",
        "        curl \\\n",
        "        git \\\n",
        "        gnupg2 \\\n",
        "        make \\\n",
        "        cmake \\\n",
        "        ffmpeg \\\n",
        "        swig \\\n",
        "        libz-dev \\\n",
        "        unzip \\\n",
        "        zlib1g-dev \\\n",
        "        libglfw3 \\\n",
        "        libglfw3-dev \\\n",
        "        libxrandr2 \\\n",
        "        libxinerama-dev \\\n",
        "        libxi6 \\\n",
        "        libxcursor-dev \\\n",
        "        libgl1-mesa-dev \\\n",
        "        libgl1-mesa-glx \\\n",
        "        libglew-dev \\\n",
        "        libosmesa6-dev \\\n",
        "        lsb-release \\\n",
        "        ack-grep \\\n",
        "        patchelf \\\n",
        "        wget \\\n",
        "        xpra \\\n",
        "        xserver-xorg-dev \\\n",
        "        xvfb \\\n",
        "        python-opengl \\\n",
        "        ffmpeg\n",
        "!pip -q install gdown\n",
        "\n",
        "# Installing dependencies for visualization\n",
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip -q install -U gym==0.19\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iviS2zbCjLhf"
      },
      "outputs": [],
      "source": [
        "#@title Download MuJoCo\n",
        "\n",
        "if not os.path.exists(f'{MJC_PATH}/mujoco210'):\n",
        "    %cd $MJC_PATH\n",
        "    !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\n",
        "    !tar -xzf mujoco210-linux-x86_64.tar.gz\n",
        "    %rm mujoco210-linux-x86_64.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fTr_34KVjYBf"
      },
      "outputs": [],
      "source": [
        "#@title Install MuJoCo\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco210/bin'.format(MJC_PATH)\n",
        "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco210'.format(MJC_PATH)\n",
        "\n",
        "# Installation on colab does not find *.so files in LD_LIBRARY_PATH,\n",
        "# copy over manually instead.\n",
        "!cp $MJC_PATH/mujoco210/bin/*.so /usr/lib/x86_64-linux-gnu/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k5lZMHjkja-G"
      },
      "outputs": [],
      "source": [
        "#@title Clone and install mujoco-py\n",
        "\n",
        "if not os.path.exists(f'{MJC_PATH}/mujoco-py'):\n",
        "    %cd $MJC_PATH\n",
        "    !git clone https://github.com/openai/mujoco-py.git\n",
        "\n",
        "%cd $MJC_PATH/mujoco-py\n",
        "!git checkout f1312cceeeebbba17e78d5d77fbffa091eed9a3a # Tested version\n",
        "%pip install -e .\n",
        "\n",
        "# Compile at the first import\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib/nvidia'\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/content/gdrive/MyDrive/rl_lab_2022/imitation_learning/mujoco/mujoco210/bin'\n",
        "import mujoco_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WrJsU52ZRnpL"
      },
      "outputs": [],
      "source": [
        "#@title Download the expert checkpoint\n",
        "\n",
        "if not os.path.exists(f'{LAB_PATH}/expert_checkpoint'):\n",
        "    %cd $LAB_PATH\n",
        "    !gdown --id 1CNhGwvqsLd-H0dwh-4L9rEqIo04CyOLW\n",
        "    !unzip expert_checkpoint.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJQ5DQFu9jMN"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import time\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Start virtual display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "# Seed random generators\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Helpers\n",
        "\n",
        "def show_video(file_name):\n",
        "    mp4 = open(file_name,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=480 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)\n",
        "\n",
        "def mlp(input_shape, output_size, hidden_sizes=(256, 256), hidden_activation=tf.tanh, output_activation=None, l2_weight=0.0001):\n",
        "    \"\"\"Creates MLP with the specified parameters.\"\"\"\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(tf.keras.Input(shape=input_shape))\n",
        "    for h in hidden_sizes:\n",
        "        model.add(tf.keras.layers.Dense(units=h,\n",
        "                                        activation=hidden_activation,\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(l2_weight)))\n",
        "    model.add(tf.keras.layers.Dense(units=output_size, activation=output_activation))\n",
        "\n",
        "    return model\n",
        "\n",
        "def run_policy (env, model, total_steps=10000, verbose=True):\n",
        "    obs_array = np.empty([total_steps, *env.observation_space.shape])\n",
        "    act_array = np.empty([total_steps, env.action_space.shape[0]])\n",
        "    rew_array = np.empty([total_steps, 1])\n",
        "    done_array = np.empty([total_steps, 1])\n",
        "\n",
        "    iter_time = time.time()\n",
        "    done = True\n",
        "    for i in range(total_steps):\n",
        "        if verbose and (i + 1) % 1000 == 0:\n",
        "            steps_per_second = 1000 / (time.time() - iter_time)\n",
        "            print(f'Step {i + 1}/{total_steps}, Steps per second: {steps_per_second}')\n",
        "            iter_time = time.time()\n",
        "\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "\n",
        "        act = model(tf.expand_dims(obs, axis=0))[0]\n",
        "        obs_, rew, done, _ = env.step(act)\n",
        "        \n",
        "        obs_array[i] = obs\n",
        "        act_array[i] = act\n",
        "        rew_array[i] = rew\n",
        "        done_array[i] = float(done)\n",
        "\n",
        "        obs = obs_\n",
        "\n",
        "    return obs_array, act_array, rew_array, done_array\n",
        "\n",
        "def calculate_returns(rew, done):\n",
        "    rew_cumsum = np.cumsum(rew)[:, None]\n",
        "    ret_cumsum = rew_cumsum * done\n",
        "    ret_cumsum_trimed = ret_cumsum[np.nonzero(ret_cumsum)]\n",
        "    ret_cumsum_trimed[1:] -= ret_cumsum_trimed[:-1]\n",
        "    return ret_cumsum_trimed\n",
        "\n",
        "def evaluate_agent(env, model, verbose=False):\n",
        "    _, _, rew, done = run_policy(env, model, total_steps=25000, verbose=verbose)\n",
        "    rets = calculate_returns(rew, done)\n",
        "\n",
        "    print(f'Num. episodes: {len(rets)}')\n",
        "    print(f'Avg. return: {np.mean(rets)}')\n",
        "    print(f'Max. return: {np.max(rets)}')\n",
        "    print(f'Min. return: {np.min(rets)}')\n",
        "\n",
        "def render_agent(env, model):\n",
        "    envw = gym.wrappers.Monitor(env, \"./\", force=True)\n",
        "    o, d = envw.reset(), False\n",
        "    while not d:\n",
        "        envw.render()\n",
        "        o, _, d, _ = envw.step(model(tf.expand_dims(o, axis=0))[0])\n",
        "    # envw.close()\n",
        "\n",
        "    file_name = glob.glob('openaigym.video.*.mp4')[0]\n",
        "    return show_video(file_name)\n",
        "\n",
        "class Expert:\n",
        "    \"\"\"Streamlined Off-Policy (SOP) actor\"\"\"\n",
        "\n",
        "    def __init__(self, ckpt_path):\n",
        "        self.model = tf.keras.models.load_model(ckpt_path)\n",
        "\n",
        "    def __call__(self, obs, exploratory=False):\n",
        "        # We need to add one more dim. for this model\n",
        "        mu, pi = self.model(tf.expand_dims(obs, axis=0))\n",
        "        return pi[0] if exploratory else mu[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFL5JXB_ON4i"
      },
      "source": [
        "## 0. Ant\n",
        "\n",
        "a three-dimensional quadrupedal robot.\n",
        "\n",
        "- Observations are 111-dim. vectors that describe the kinematic properties of the robot,\n",
        "- Actions are 8-dim. vectors which specify torques to be applied on the robot joints,\n",
        "- The goal is to run forward as fast as possible and don’t fall over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFBJkqch9mQF"
      },
      "outputs": [],
      "source": [
        "%cd $LAB_PATH\n",
        "expert = Expert('expert_checkpoint')\n",
        "env = gym.make('Ant-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MkDfFf_Oe_D"
      },
      "outputs": [],
      "source": [
        "#render_agent(env, expert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptP_oy7WfmLq"
      },
      "outputs": [],
      "source": [
        "evaluate_agent(env, expert, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZEdjUd8fQPq"
      },
      "source": [
        "## 1. Behaviour Clonning\n",
        "\n",
        "1. Collect the expert data.\n",
        "2. Fit the model (classifier/regressor) to the expert data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1Ii0px3TMIo"
      },
      "outputs": [],
      "source": [
        "# Collect the expert data\n",
        "# obs, act, _, _ = run_policy(env, expert, total_steps=100000)\n",
        "\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1-0FtkebJvIZ0NUTftMyRavqkV_b0nDVl')\n",
        "\n",
        "with np.load('expert_greedy_data.npz') as data:\n",
        "    obs, act, _, _ = data.values()\n",
        "\n",
        "if not os.path.exists(f'expert_exploratory_data.npz'):\n",
        "    !gdown --id 1-9C1hdDY7Q3ckBY3ToVF2VF-BtcoMhQN\n",
        "    \n",
        "with np.load('expert_exploratory_data.npz') as data:\n",
        "    obs_expl, act_expl, rew_expl, done_expl = data.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k20RkXkgRQUE"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Create the imitator model observations -> actions\n",
        "imitator = mlp(111, 8)\n",
        "\n",
        "# We will start our experiments from the same weights for the fair comparison\n",
        "init_weights = imitator.get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT6V6Eq7kZQa"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Fit the model to the expert data\n",
        "imitator.set_weights(init_weights)\n",
        "\n",
        "# ANSWER\n",
        "imitator.compile(loss=tf.keras.losses.MeanSquaredError())\n",
        "imitator.fit(obs, act, epochs=25)\n",
        "# END ANSWER\n",
        "\n",
        "evaluate_agent(env, imitator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxzMvc4BSSWA"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Discuss the questions\n",
        "\n",
        "1. In principle, do we need the expert policy for BC?\n",
        "\n",
        "  > Answer: ...\n",
        "\n",
        "1. What are the problems with BC?\n",
        "\n",
        "  > Answer: ...\n",
        "\n",
        "1. How can we help BC do better?\n",
        "\n",
        "  > Answer: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nqU9GHTwROv"
      },
      "outputs": [],
      "source": [
        "# Collect the exploratory data\n",
        "def exploratory(obs):\n",
        "    \"\"\"Adds the Gaussian noise to the expert actions.\"\"\"\n",
        "    act = expert(obs)\n",
        "    return act + 0.29 * tf.random.normal(tf.shape(act))\n",
        "# obs_expl, act_expl, rew_expl, done_expl = run_policy(env, exploratory, total_steps=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7s1_wVnD8ONV"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Run BC on the exploratory data\n",
        "\n",
        "imitator.set_weights(init_weights)\n",
        "\n",
        "# ANSWER\n",
        "imitator.compile(loss=tf.keras.losses.MeanSquaredError())\n",
        "imitator.fit(obs_expl, act_expl, epochs=25)\n",
        "# END ANSWER\n",
        "\n",
        "evaluate_agent(env, imitator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRJufsMEVfEa"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Answer the questions\n",
        "\n",
        "1. Why does it better?\n",
        "\n",
        "  > Answer: ...\n",
        "\n",
        "1. How can we use the expert to further improve the data?\n",
        "\n",
        "  > Hint: Noisy actions help in collecting more diverse data, but we don't want to learn the exploratory actions.\n",
        "\n",
        "  > Answer: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFnAHYtnyAGc"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Infere the expert actions on the exploratory observations\n",
        "#           and run BC on it.\n",
        "\n",
        "imitator.set_weights(init_weights)\n",
        "\n",
        "# ANSWER\n",
        "imitator.compile(loss=tf.keras.losses.MeanSquaredError())\n",
        "imitator.fit(obs_expl, act_expl, epochs=25)\n",
        "# ANSWER END\n",
        "\n",
        "evaluate_agent(env, imitator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgIQLrNEVnRW"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Answer the questions\n",
        "\n",
        "1. Did it help? Why?\n",
        "\n",
        "  > Answer: ...\n",
        "\n",
        "1. How can you extend this idea?\n",
        "\n",
        "  > Hint: How can we get more exploratory data?\n",
        "\n",
        "  > Answer: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M99bZtXz3ru"
      },
      "source": [
        "## 2. Imitation Learning via Interactive Demostrator\n",
        "\n",
        "[DAgger](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)\n",
        "\n",
        "1. Collect the expert data.\n",
        "2. Fit the model (classifier/regressor) to the expert data.\n",
        "3. Collect the imitator data.\n",
        "4. Infere the expert actions on the imitator data.\n",
        "5. Fit the model to the extended dataset.\n",
        "6. Repeat from 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLhHpI3bk2gM"
      },
      "outputs": [],
      "source": [
        "# We will pre-train on less expert data to keep the same dataset size\n",
        "obs_ = obs[:30000,:]\n",
        "act_ = act[:30000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDOPaUetz6-1"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Pretrain for 25 epochs\n",
        "\n",
        "imitator.set_weights(init_weights)\n",
        "\n",
        "# ANSWER\n",
        "imitator.compile(loss=tf.keras.losses.MeanSquaredError())\n",
        "imitator.fit(obs_, act_, epochs=25)\n",
        "# END ANSWER\n",
        "\n",
        "evaluate_agent(env, imitator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHk7Fmau3Jnn"
      },
      "outputs": [],
      "source": [
        "# Exercise: Implement DAgger\n",
        "\n",
        "for i in range(7):\n",
        "    print(f'\\n### Iter. {i+1} ###')\n",
        "\n",
        "    # ANSWER\n",
        "    print('\\n1. Data collection')\n",
        "    obs_extra, _, _, _ = run_policy(env, imitator, total_steps=10000) # Collect 10k steps\n",
        "   \n",
        "    obs_ = np.cat((obs_, obs_extra))\n",
        "    act_ = np.cat((act_, expert(obs_extra)))\n",
        "\n",
        "    print('\\n2. Training')\n",
        "    imitator.set_weights(init_weights)\n",
        "\n",
        "    imitator.compile(loss=tf.keras.losses.MeanSquaredError())\n",
        "    imitator.fit(obs_, act_)\n",
        "    \n",
        "    # END ANSWER\n",
        "\n",
        "    print('\\n3. Evaluation')\n",
        "    evaluate_agent(env, imitator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzn9oXqZUoiA"
      },
      "source": [
        "### Note\n",
        "\n",
        "Training the expert with the SOP algorithm (Wang et al., 2020) took 3M data samples (env. interactions). Here, we nearly match it with only 100k samples! Training from the expert can be much more efficient than reinforcement learning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}